{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython.display import Image, HTML\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "def display_images(image_paths):\n",
    "    html_content = \"<div width='100%'>\"\n",
    "    for image_path in image_paths:\n",
    "        html_content += '<div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">\\\n",
    "         {image_path}:\\\n",
    "         <img src=\"{image_path}\" style=\"display:inline-block;\"> </div>'.format(image_path=image_path)\n",
    "    html_content += '</div>'\n",
    "    display(HTML(html_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. setting base configuration ..\n"
     ]
    }
   ],
   "source": [
    "print(\".. setting base configuration ..\")\n",
    "\n",
    "random.seed(34283428)\n",
    "base_dir_path = '.'\n",
    "best_model_filepath = '{}/saved-models/model1.hdf5'.format(base_dir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. setting model parameters ..\n"
     ]
    }
   ],
   "source": [
    "print(\".. setting model parameters ..\")\n",
    "\n",
    "training_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. loading & splitting data ..\n"
     ]
    }
   ],
   "source": [
    "print(\".. loading & splitting data ..\")\n",
    "section_start_time = datetime.datetime.utcnow()\n",
    "\n",
    "image_to_label = {}\n",
    "images_list = []\n",
    "label_list = []\n",
    "with open('{base_dir_path}/data-labels/images.csv'.format(base_dir_path=base_dir_path),\n",
    "          newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        image_name = row['IMAGE FILENAME'].strip()\n",
    "        is_huddle = 1 if row['IS HUDDLE'] else 0\n",
    "        if image_name:\n",
    "            image_path = '{base_dir_path}/data-images/{image_name}'.format(base_dir_path=base_dir_path,\n",
    "                                                                           image_name=image_name)\n",
    "            image_to_label[image_path] = is_huddle\n",
    "            images_list.append(image_path)\n",
    "            label_list.append(is_huddle)\n",
    "\n",
    "label_list_categorical = to_categorical(label_list)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(images_list),\n",
    "                                                    np.array(label_list_categorical),\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train,\n",
    "                                                            y_train,\n",
    "                                                            test_size=0.20,\n",
    "                                                            random_state=42)\n",
    "\n",
    "train_tensors = paths_to_tensor(X_train).astype('float32')\n",
    "test_tensors = paths_to_tensor(X_test).astype('float32')\n",
    "valid_tensors = paths_to_tensor(X_validate).astype('float32')\n",
    "\n",
    "duration_loading = (datetime.datetime.utcnow() - section_start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. constructing the model ..\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 224, 224, 16)      208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 224, 224, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 224, 224, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 112, 112, 32)      2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 11,122\n",
      "Trainable params: 10,898\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\".. constructing the model ..\")\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', input_shape=(224, 224, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. training the model ..\n",
      "Train on 896 samples, validate on 224 samples\n",
      "Epoch 1/1\n",
      "896/896 [==============================] - 118s 131ms/step - loss: 0.4257 - acc: 0.7924 - val_loss: 0.3182 - val_acc: 0.8661\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.31816, saving model to ./saved-models/model1.hdf5\n"
     ]
    }
   ],
   "source": [
    "print(\".. training the model ..\")\n",
    "section_start_time = datetime.datetime.utcnow()\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=best_model_filepath,\n",
    "                               verbose=1,\n",
    "                               save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, y_train,\n",
    "          validation_data=(valid_tensors, y_validate),\n",
    "          epochs=training_epochs,\n",
    "          batch_size=20,\n",
    "          callbacks=[checkpointer],\n",
    "          verbose=1)\n",
    "\n",
    "duration_training = (datetime.datetime.utcnow() - section_start_time).total_seconds()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. loading best weights ..\n",
      ".. testing the model ..\n",
      ".. reporting results ..\n"
     ]
    }
   ],
   "source": [
    "print(\".. loading best weights ..\")\n",
    "\n",
    "model.load_weights(best_model_filepath)\n",
    "\n",
    "print(\".. testing the model ..\")\n",
    "section_start_time = datetime.datetime.utcnow()\n",
    "\n",
    "tmp_predictions = [model.predict(np.expand_dims(tensor, axis=0)) for tensor in test_tensors]\n",
    "test_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "duration_testing = (datetime.datetime.utcnow() - section_start_time).total_seconds()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. analyzing results ..\n"
     ]
    }
   ],
   "source": [
    "print(\".. analyzing results ..\")\n",
    "\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "true_negatives = 0\n",
    "false_negatives = 0\n",
    "all_positives = 0\n",
    "all_test = len(X_test)\n",
    "\n",
    "false_positive_images = []\n",
    "false_negative_images = []\n",
    "\n",
    "for n, test_image in enumerate(X_test):\n",
    "    prediction_label = True if test_predictions[n] else False\n",
    "    truth_label = True if y_test[n][1] else False\n",
    "    correct_prediction = prediction_label == truth_label\n",
    "\n",
    "#     print(\"{indicator} {truth_icon} {test_image} (is {truth} predicted {prediction})\".format(\n",
    "#         indicator=' ' if correct_prediction else 'X',\n",
    "#         truth_icon='|' if truth_label else '=',\n",
    "#         test_image=test_image,\n",
    "#         truth=truth_label,\n",
    "#         prediction=prediction_label,\n",
    "#     ))\n",
    "\n",
    "    if truth_label:\n",
    "        all_positives += 1\n",
    "\n",
    "    if prediction_label:\n",
    "        if truth_label:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "            false_positive_images.append(test_image)\n",
    "    else:\n",
    "        if truth_label:\n",
    "            false_negatives += 1\n",
    "            false_negative_images.append(test_image)\n",
    "        else:\n",
    "            true_negatives += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________\n",
      "SUMMARY:\n",
      "\n",
      "loading duration: 9.0 seconds\n",
      "training duration: 119.5 seconds\n",
      "testing duration: 35.3 seconds\n",
      "all:  280\n",
      "all_positives:  58\n",
      "true_positives:  37\n",
      "true_negatives:  210\n",
      "false_positives:  12\n",
      "false_negatives:  21\n",
      "RECALL: 63.79%\n",
      "ACCURACY: 88.21%\n"
     ]
    }
   ],
   "source": [
    "print(\"________________________________________________________\")\n",
    "print(\"SUMMARY:\\n\")\n",
    "print(\"loading duration: {0:.1f} seconds\".format(duration_loading))\n",
    "print(\"training duration: {0:.1f} seconds\".format(duration_training))\n",
    "print(\"testing duration: {0:.1f} seconds\".format(duration_testing))\n",
    "\n",
    "print(\"all: \", all_test)\n",
    "print(\"all_positives: \", all_positives)\n",
    "print(\"true_positives: \", true_positives)\n",
    "print(\"true_negatives: \", true_negatives)\n",
    "print(\"false_positives: \", false_positives)\n",
    "print(\"false_negatives: \", false_negatives)\n",
    "\n",
    "print(\"RECALL: {0:.2f}%\".format(100 * true_positives / all_positives))\n",
    "print(\"ACCURACY: {0:.2f}%\".format(100 * (true_positives + true_negatives) / all_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Positives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div width='100%'><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831138807001-1266.jpg:         <img src=\"./data-images/5831138807001-1266.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5833090951001-1429.jpg:         <img src=\"./data-images/5833090951001-1429.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831137888001-531.jpg:         <img src=\"./data-images/5831137888001-531.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5833090951001-1622.jpg:         <img src=\"./data-images/5833090951001-1622.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831135399001-1894.jpg:         <img src=\"./data-images/5831135399001-1894.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5833090951001-1397.jpg:         <img src=\"./data-images/5833090951001-1397.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831138807001-554.jpg:         <img src=\"./data-images/5831138807001-554.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831139883001-1780.jpg:         <img src=\"./data-images/5831139883001-1780.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5833090951001-1722.jpg:         <img src=\"./data-images/5833090951001-1722.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5833090951001-461.jpg:         <img src=\"./data-images/5833090951001-461.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5833096735001-2172.jpg:         <img src=\"./data-images/5833096735001-2172.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5833090951001-727.jpg:         <img src=\"./data-images/5833090951001-727.jpg\" style=\"display:inline-block;\"> </div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_images(false_positive_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Negatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div width='100%'><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831139883001-224.jpg:         <img src=\"./data-images/5831139883001-224.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831139883001-1337.jpg:         <img src=\"./data-images/5831139883001-1337.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831138807001-1167.jpg:         <img src=\"./data-images/5831138807001-1167.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5833084561001-1562.jpg:         <img src=\"./data-images/5833084561001-1562.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831135399001-2347.jpg:         <img src=\"./data-images/5831135399001-2347.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5833084561001-1887.jpg:         <img src=\"./data-images/5833084561001-1887.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831138807001-957.jpg:         <img src=\"./data-images/5831138807001-957.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831139883001-1904.jpg:         <img src=\"./data-images/5831139883001-1904.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5833084561001-804.jpg:         <img src=\"./data-images/5833084561001-804.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831138807001-353.jpg:         <img src=\"./data-images/5831138807001-353.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831139883001-1828.jpg:         <img src=\"./data-images/5831139883001-1828.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831139883001-1953.jpg:         <img src=\"./data-images/5831139883001-1953.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831137888001-888.jpg:         <img src=\"./data-images/5831137888001-888.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5833084561001-196.jpg:         <img src=\"./data-images/5833084561001-196.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831137888001-1564.jpg:         <img src=\"./data-images/5831137888001-1564.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831138807001-1837.jpg:         <img src=\"./data-images/5831138807001-1837.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5833096735001-1311.jpg:         <img src=\"./data-images/5833096735001-1311.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831135399001-2343.jpg:         <img src=\"./data-images/5831135399001-2343.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5833084561001-1906.jpg:         <img src=\"./data-images/5833084561001-1906.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5833096735001-580.jpg:         <img src=\"./data-images/5833096735001-580.jpg\" style=\"display:inline-block;\"> </div><div style=\"font-size: 10px; display:inline-block; width: 224px; border:1px solid black\">         ./data-images/5831137888001-132.jpg:         <img src=\"./data-images/5831137888001-132.jpg\" style=\"display:inline-block;\"> </div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_images(false_negative_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
